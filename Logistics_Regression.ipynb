{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression (SLR)? Explain its purpose.\n",
        "\n",
        "  Simple Linear Regression (SLR) is a statistical method used to study the relationship between one independent variable and one dependent variable. It helps in understanding how changes in one variable bring changes in another variable. In this method, the relationship between the two variables is represented by a straight line, which is called the regression line.\n",
        "\n",
        "The mathematical form of Simple Linear Regression is expressed as:\n",
        "\n",
        "Y = a + bX + e\n",
        "\n",
        "Here, Y represents the dependent variable, which is the variable we want to explain or predict. X represents the independent variable, which is the variable that influences Y. The term “a” is known as the intercept, which shows the value of Y when X is equal to zero. The term “b” is called the slope coefficient, which indicates how much Y changes when X increases by one unit. The term “e” represents the error term, which includes other factors affecting Y that are not included in the model.\n",
        "\n",
        "The main purpose of Simple Linear Regression is to measure the relationship between two variables. It helps in identifying whether the relationship is positive, negative, or nonexistent. If the slope coefficient (b) is positive, it means that as X increases, Y also increases. If the slope coefficient is negative, it means that as X increases, Y decreases. If the slope is zero, there is no relationship between the variables.\n",
        "\n",
        "Another important purpose of SLR is prediction. Once the regression equation is estimated, we can use it to predict the value of the dependent variable for any given value of the independent variable. For example, it can be used to predict sales based on advertising expenditure, income based on years of education, or consumption based on income. This makes it very useful in economics, business, and social sciences.\n",
        "\n",
        "Simple Linear Regression also helps in measuring the strength of the effect of the independent variable on the dependent variable. The slope coefficient clearly shows how much change occurs in Y for a one-unit change in X. In addition, statistical tests such as the t-test and the coefficient of determination (R²) are used to check whether the relationship is statistically significant and how well the model explains the variation in the dependent variable.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iTOc9hIfGBpM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "The key assumptions of **Simple Linear Regression (SLR)** are conditions that must be satisfied for the regression results to be reliable and valid. These assumptions ensure that the estimated coefficients are unbiased and that hypothesis tests are meaningful.\n",
        "\n",
        "The main assumptions are explained below in text form:\n",
        "\n",
        "**1. Linearity**\n",
        "There must be a linear relationship between the independent variable (X) and the dependent variable (Y). This means that the change in Y is proportional to the change in X, and the relationship can be represented by a straight line. If the relationship is curved or nonlinear, Simple Linear Regression is not appropriate.\n",
        "\n",
        "**2. Independence of Errors**\n",
        "The error terms (residuals) must be independent of each other. This means that the error in one observation should not influence the error in another observation. This assumption is especially important in time series data, where values may be related over time.\n",
        "\n",
        "**3. Homoscedasticity (Constant Variance of Errors)**\n",
        "The variance of the error terms should remain constant for all values of X. In other words, the spread of residuals should be roughly the same across all levels of the independent variable. If the variance changes (i.e., increases or decreases with X), the problem is called heteroscedasticity, which can affect the reliability of statistical tests.\n",
        "\n",
        "**4. Normality of Errors**\n",
        "The error terms should be normally distributed. This assumption is mainly important for conducting hypothesis tests and constructing confidence intervals. If the sample size is large, this assumption becomes less critical due to the Central Limit Theorem.\n",
        "\n",
        "**5. No Perfect Multicollinearity**\n",
        "In Simple Linear Regression, there is only one independent variable, so perfect multicollinearity is generally not an issue. However, the independent variable should vary in the dataset and should not be a constant value. There must be sufficient variation in X to estimate the slope.\n",
        "\n",
        "**6. Zero Mean of Errors**\n",
        "The average value of the error terms should be zero. This means that the model does not systematically overestimate or underestimate the dependent variable.\n",
        "\n"
      ],
      "metadata": {
        "id": "eZoufUZMGdAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write the mathematical equation for a simple linear regression model and explain each term.\n",
        "\n",
        "The mathematical equation for a **Simple Linear Regression (SLR) model** is:\n",
        "\n",
        "[Y = a + bX + e]\n",
        "\n",
        "In some textbooks, it is also written as:\n",
        "\n",
        "[Y_i = β0 + β1xi + ui]\n",
        "\n",
        "Both forms represent the same idea. The second form is the more general statistical notation.\n",
        "\n",
        "### Explanation of Each Term\n",
        "\n",
        "**1. (Y) (Dependent Variable)**\n",
        "This is the variable we want to explain or predict. It depends on the independent variable. For example, consumption, sales, income, or marks obtained can be the dependent variable.\n",
        "\n",
        "**2. (X) (Independent Variable)**\n",
        "This is the explanatory variable that influences the dependent variable. It is used to predict or explain changes in Y. For example, income (when explaining consumption), advertising expenditure (when explaining sales), or hours studied (when explaining marks).\n",
        "\n",
        "**3. (a) or (β0) (Intercept or Constant Term)**\n",
        "This represents the value of Y when X is equal to zero. It shows the starting point of the regression line on the Y-axis. In practical situations, it may or may not have meaningful economic interpretation, depending on the context.\n",
        "\n",
        "**4. (b) or (β1) (Slope Coefficient)**\n",
        "This measures the rate of change in Y for a one-unit change in X. It shows the direction and strength of the relationship:\n",
        "\n",
        "* If (b > 0), the relationship is positive.\n",
        "* If (b < 0), the relationship is negative.\n",
        "* If (b = 0), there is no relationship.\n",
        "\n",
        "**5. (e) or (ui) (Error Term)**\n",
        "This represents all other factors that affect Y but are not included in the model. It captures random disturbances, measurement errors, and omitted variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "bkicHbi6Gmo5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Provide a real-world example where simple linear regression can be applied.\n",
        "\n",
        "A common real-world example where **Simple Linear Regression (SLR)** can be applied is in analyzing the relationship between **advertising expenditure and sales revenue**.\n",
        "\n",
        "Suppose a company wants to understand how its spending on advertising affects its sales. Here:\n",
        "\n",
        "* **Independent Variable (X):** Advertising expenditure\n",
        "* **Dependent Variable (Y):** Sales revenue\n",
        "\n",
        "The company collects data for several months on how much it spent on advertising and the corresponding sales achieved. Using Simple Linear Regression, the company can estimate an equation such as:\n",
        "\n",
        "[\n",
        "Sales = a + b(Advertising\\ Expenditure)\n",
        "]\n",
        "\n",
        "If the estimated equation is:\n",
        "\n",
        "[\n",
        "Sales = 50,000 + 5X\n",
        "]\n",
        "\n",
        "This means:\n",
        "\n",
        "* When advertising expenditure is zero, sales are expected to be 50,000 (base sales).\n",
        "* For every additional one unit increase in advertising spending (for example, ₹1,000), sales increase by 5 units (for example, ₹5,000), depending on how the variables are measured.\n",
        "\n",
        "Why SLR is Useful in This Example\n",
        "\n",
        "1. It helps measure whether advertising has a positive or negative impact on sales.\n",
        "2. It helps predict future sales based on planned advertising budgets.\n",
        "3. It assists management in making better marketing and budgeting decisions.\n",
        "\n",
        "Thus, Simple Linear Regression provides a practical and effective way to analyze and predict relationships between two related variables in real-world business situations.\n"
      ],
      "metadata": {
        "id": "IUU_h4fKHZaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is the method of least squares in linear regression?\n",
        "\n",
        "\n",
        "The method of least squares is a statistical technique used in linear regression to estimate the values of the regression coefficients. It is used to determine the best-fitting straight line that represents the relationship between the independent variable and the dependent variable. The basic idea of this method is to minimize the total error between the actual observed values and the values predicted by the regression line.\n",
        "\n",
        "In Simple Linear Regression, the model is written as Y = a + bX + e, where Y is the dependent variable, X is the independent variable, a is the intercept, b is the slope, and e is the error term. For each observation, there is a difference between the actual value of Y and the predicted value obtained from the regression equation. This difference is called the residual or error.\n",
        "\n",
        "The method of least squares works by minimizing the sum of the squares of these residuals. The errors are squared because positive and negative errors would otherwise cancel each other out. Squaring also gives more weight to larger errors. By minimizing the total squared errors, we obtain the most accurate estimates of the intercept and slope.\n",
        "\n",
        "Using this method, the slope (b) is calculated based on the relationship between deviations of X and Y from their respective means. The intercept (a) is then determined using the mean values of X and Y. The resulting regression line is the one that has the smallest possible total squared error.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_H8ExFEQHpsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is Logistic Regression? How does it differ from Linear Regression?\n",
        "\n",
        "Logistic Regression is a statistical method used when the dependent variable is categorical, usually binary in nature, such as yes/no, success/failure, or pass/fail. It is mainly used to predict the probability that a certain event will occur. Instead of predicting a continuous numerical value, logistic regression estimates the probability of an outcome and ensures that the predicted values lie between 0 and 1.\n",
        "\n",
        "In logistic regression, the relationship between the independent variable and the dependent variable is not represented by a straight line. Instead, it is represented by an S-shaped curve, known as the logistic or sigmoid curve. The model estimates the probability of the occurrence of an event by transforming the linear equation into a probability using a logistic function. Logistic regression actually works with the log-odds of the probability rather than the probability directly. The parameters of the model are estimated using the method of maximum likelihood rather than the method of least squares.\n",
        "\n",
        "Logistic Regression differs from Linear Regression in several important ways. First, linear regression is used when the dependent variable is continuous, such as income, sales, or marks, whereas logistic regression is used when the dependent variable is categorical or binary. Second, linear regression produces predicted values that can range from negative infinity to positive infinity, while logistic regression produces probabilities that always lie between 0 and 1. Third, linear regression fits a straight line to the data, whereas logistic regression fits an S-shaped curve. Finally, linear regression uses the method of least squares to estimate parameters, while logistic regression uses maximum likelihood estimation.\n",
        "\n"
      ],
      "metadata": {
        "id": "0MsPX0PDI_46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Name and briefly describe three common evaluation metrics for regression models.\n",
        "\n",
        "Three common evaluation metrics for regression models are explained below:\n",
        "\n",
        "**1. Mean Absolute Error (MAE)**\n",
        "Mean Absolute Error measures the average of the absolute differences between the actual values and the predicted values. It tells us how far, on average, the predictions are from the true values, without considering the direction of the errors. Since it uses absolute values, negative and positive errors do not cancel out. A smaller MAE indicates better model performance. It is easy to understand because it is expressed in the same units as the dependent variable.\n",
        "\n",
        "**2. Mean Squared Error (MSE)**\n",
        "Mean Squared Error measures the average of the squared differences between actual and predicted values. By squaring the errors, larger errors are given more weight than smaller ones. This makes MSE particularly useful when large errors are especially undesirable. However, because the errors are squared, the unit of MSE is the square of the original unit, which can make interpretation less intuitive. A lower MSE indicates a better-fitting model.\n",
        "\n",
        "**3. R-squared (Coefficient of Determination)**\n",
        "R-squared measures the proportion of the total variation in the dependent variable that is explained by the independent variable(s) in the model. Its value ranges from 0 to 1. A value closer to 1 indicates that the model explains a large portion of the variation in the data, while a value closer to 0 indicates poor explanatory power. R-squared helps in understanding how well the model fits the data overall.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7cwuJI55JiKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  What is the purpose of the R-squared metric in regression analysis?\n",
        "\n",
        "The purpose of the R-squared metric in regression analysis is to measure how well the regression model explains the variation in the dependent variable. It indicates the proportion of the total variation in the dependent variable that is explained by the independent variable or variables included in the model.\n",
        "\n",
        "R-squared, also known as the coefficient of determination, takes values between 0 and 1. A value close to 1 means that a large portion of the variation in the dependent variable is explained by the model, indicating a good fit. A value close to 0 means that the model explains only a small portion of the variation, indicating a poor fit.\n",
        "\n",
        "For example, if the R-squared value is 0.80, it means that 80 percent of the variation in the dependent variable is explained by the independent variable(s), while the remaining 20 percent is due to other factors not included in the model or random error.\n",
        "\n",
        "The main purpose of R-squared is therefore to evaluate the overall explanatory power and goodness of fit of a regression model. However, a high R-squared does not necessarily mean that the model is perfect or that it has strong predictive power, as it does not measure whether the relationship is statistically significant or whether the model is appropriate.\n"
      ],
      "metadata": {
        "id": "xEOKvTAHJv9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write Python code to fit a simple linear regression model using scikit-learn and print the slope and intercept.\n",
        "\n"
      ],
      "metadata": {
        "id": "fKT5-evkJ7d2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data (Independent variable X and Dependent variable y)\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([2, 4, 5, 4, 5])\n",
        "\n",
        "# Create the model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get slope (coefficient) and intercept\n",
        "slope = model.coef_[0]\n",
        "intercept = model.intercept_\n",
        "\n",
        "# Print results\n",
        "print(\"Slope (Coefficient):\", slope)\n",
        "print(\"Intercept:\", intercept)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5Rp5SuJKCYu",
        "outputId": "cbdf1a3f-1c09-4646-e076-7086d57a07f8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope (Coefficient): 0.6\n",
            "Intercept: 2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do you interpret the coefficients in a simple linear regression model?\n",
        "\n",
        "In a Simple Linear Regression model, the equation is written as:\n",
        "\n",
        "Y = a + bX\n",
        "\n",
        "The coefficients in this equation are the **intercept (a)** and the **slope (b)**. Each has a specific interpretation.\n",
        "\n",
        "The **intercept (a)** represents the value of the dependent variable (Y) when the independent variable (X) is equal to zero. It shows the starting point of the regression line on the Y-axis. In some practical situations, the intercept has a meaningful interpretation. For example, if we are studying the relationship between advertising expenditure and sales, the intercept may represent the expected sales when no money is spent on advertising. However, if X = 0 is not realistic in the given context, the intercept may not have a strong practical meaning, though it is still important mathematically.\n",
        "\n",
        "The **slope (b)** represents the rate of change in the dependent variable for a one-unit increase in the independent variable. In other words, it shows how much Y is expected to increase or decrease when X increases by one unit. If the slope is positive, it indicates a positive relationship between X and Y, meaning that as X increases, Y also increases. If the slope is negative, it indicates a negative relationship, meaning that as X increases, Y decreases. If the slope is zero, it suggests that there is no linear relationship between the variables.\n",
        "\n",
        "For example, if the estimated regression equation is:\n",
        "\n",
        "Y = 10 + 3X\n",
        "\n",
        "This means that when X is zero, Y is expected to be 10. It also means that for every one-unit increase in X, Y increases by 3 units on average.\n",
        "\n",
        "In summary, the intercept shows the baseline level of the dependent variable, and the slope measures the direction and strength of the relationship between the independent and dependent variables.\n"
      ],
      "metadata": {
        "id": "7vHFq66MKR6B"
      }
    }
  ]
}